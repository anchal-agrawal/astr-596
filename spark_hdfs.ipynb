{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=center>Introduction to Machine Learning and Data Analysis with Spark</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark is an open-source framework for fast and scalable data processing. It has built-in modules and libraries for machine learning, SQL and graph processing. Spark has high-level API in Python, Scala, Java and R. A typical Spark deployment has multiple nodes. A Spark cluster can be set up using three cluster management technologies:\n",
    "\n",
    "1. Spark standalone\n",
    "2. YARN (used by the Hadoop ecosystem)\n",
    "3. Mesos\n",
    "\n",
    "This example uses a Spark standalone cluster running 1 master and 6 slave nodes. It has 24 cores and 17.2 GB of usable memory for processing.\n",
    "\n",
    "![](cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## HDFS (Hadoop Distributed File System)\n",
    "\n",
    "For storage, this Spark cluster uses HDFS, a scalable and fault-tolerant distributed file system that's used extensively in Hadoop applications. HDFS partitions files into blocks of fixed size (usually 128 or 256 MB) and replicates them across the cluster for high availability. Files can be put in HDFS through the command line or code. To read files, we can use the HDFS URI for that cluster, followed by the file path: *hdfs://hdfs-master-ip:port/path-to-file/*\n",
    "\n",
    "The HDFS cluster in this example is running 1 master *(NameNode)* and 3 slave nodes *(DataNodes)*.\n",
    "\n",
    "![](hdfsarchitecture.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Communicating with the Spark cluster\n",
    "\n",
    "A Spark job is handled by a *SparkContext*, which is the mode of communication between the driver process (client) and the Spark executors (workers). A SparkContext requests resources from the Spark cluster and specifies the URI of the Spark cluster along with parameters such as number of cores for the job. The SparkContext can be specified as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f1091a8ba50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# stop the sparkcontext if it's already running\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "conf = SparkConf().setMaster(\"spark://10.0.3.70:7077\").setAppName(\"Intro to Spark\").set(\"spark.driver.port\", 8200).set(\"spark.cores.max\", 10)\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Working with data\n",
    "\n",
    "A *Resilient Distributed Dataset (RDD)* is the basic data abstraction in Spark. It represents a collection of data elements that can be operated upon. We can parallize a Python collection to form an RDD which can be operated upon in parallel across the Spark cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "test_list = range(100)\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To distribute a collection and form an RDD, we can use the *parallelize()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_slices = 40\n",
    "test_RDD = sc.parallelize(test_list, num_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can apply transformations to this RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320, 325, 330, 335, 340, 345, 350, 355, 360, 365, 370, 375, 380, 385, 390, 395, 400, 405, 410, 415, 420, 425, 430, 435, 440, 445, 450, 455, 460, 465, 470, 475, 480, 485, 490, 495]\n"
     ]
    }
   ],
   "source": [
    "new_RDD = test_RDD.map(lambda x: x*5)\n",
    "print(new_RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can read a file from HDFS with the *textFile()* method by supplying the HDFS URI of the file. The textFile() method returns the file as an RDD of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6855029"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flight data from http://stat-computing.org/dataexpo/2009/\n",
    "text_file = sc.textFile('hdfs://10.0.3.113:9000/home/ubuntu/data/2008.csv')\n",
    "\n",
    "# Year, Month, DayofMonth, DayOfWeek, DepartureTime, ArrivalDelay, DepartureDelay, Origin, Dest, Distance\n",
    "columns = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[3], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'NA' not in line) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "arrival_delays = columns.map(lambda p: int(p[5]))\n",
    "depart_delays = columns.map(lambda p: int(p[6]))\n",
    "distances = columns.map(lambda p: int(p[9]))\n",
    "\n",
    "arrival_delays.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can calculate correlations between variables using the *Statistics* module in *MLlib* (Spark's machine learning library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.931390780111016"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "corr1 = Statistics.corr(arrival_delays, depart_delays)\n",
    "corr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Spark Streaming\n",
    "\n",
    "Spark Streaming is an extension of the Spark platform that allows stream processing of live data. Data can be ingested from sources such as HDFS, Kafka and Twitter, and can be processed using Spark's libraries. The output can be written to filesystems (such as HDFS) and databases (such as HBase).\n",
    "\n",
    "![](streaming-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Spark Streaming Example\n",
    "\n",
    "This example counts words from a server listening on a TCP socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a StreamingContext using the SparkContext    \n",
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "from pprint import pprint\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To release the Spark cluster resources when we're done, we should stop the SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "1. Professor Brunner's notebook: https://github.com/ProfessorBrunner/rp-pdm15/blob/master/Week2/intro2spark.ipynb\n",
    "2. Spark: http://spark.apache.org/\n",
    "3. HDFS: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\n",
    "4. Spark Streaming: http://spark.apache.org/docs/latest/streaming-programming-guide.html#basic-concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
