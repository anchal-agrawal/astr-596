{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=center>Introduction to Machine Learning with Spark</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark is an open-source framework for fast and scalable data processing. It has built-in modules and libraries for machine learning, SQL and graph processing. Spark can run on multiple nodes using three cluster management technologies:\n",
    "\n",
    "1. Spark standalone\n",
    "2. YARN (used by the Hadoop ecosystem)\n",
    "3. Mesos\n",
    "\n",
    "This example uses a Spark standalone cluster running 1 master and 6 slave nodes. It has 24 cores and 17.2 GB of usable memory for processing.\n",
    "\n",
    "![](cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS (Hadoop Distributed File System)\n",
    "\n",
    "For storage, this Spark cluster uses HDFS, a scalable and fault-tolerant distributed file system that's used extensively in Hadoop applications. HDFS partitions files into blocks of fixed size (usually 128 or 256 MB) and replicates them across the cluster for high availability. Files can be put in HDFS through the command line or code. To read files, we can use the HDFS URI for that cluster, followed by the file path: *hdfs://hdfs-master-ip:port/path-to-file/*\n",
    "\n",
    "The HDFS cluster in this example is running 1 master *(NameNode)* and 3 slave nodes *(DataNodes)*.\n",
    "\n",
    "![](hdfsarchitecture.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking Python to the Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "conf = SparkConf().setMaster(\"spark://10.0.3.70:7077\").setAppName(\"Intro to Spark\").set(\"spark.driver.port\",8200).set(\"spark.cores.max\",8)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7ffafc1af890>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311827"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = sc.textFile('hdfs://10.0.3.113:9000/home/ubuntu/data/1987.csv')\n",
    "tf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To release the Spark cluster resources when we're done, we should stop the SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data in Spark\n",
    "\n",
    "\n",
    "A Resilient Distributed Dataset (RDD) is the basic data abstraction in Spark. It represents a collection of data elements that can be operated upon. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
